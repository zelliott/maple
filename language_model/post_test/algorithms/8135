
The brain is able to perform actions based on an adequate internal representation of the world, where zzso features are ignored and incomplete sensory data are zzso Traditionally, it is assumed that such abstract state representations are obtained purely from the statistics of sensory input for example by zzso learning zzso However, more recent findings suggest an influence of the zzso system, which can be modeled by a reinforcement learning zzso Standard reinforcement learning zzso act on a single layer network connecting the state space to the action zzso Here, we involve in a feature detection stage and a memory layer, which together, construct the state space for a learning zzso The memory layer consists of the state zzso at the previous time step as well as the previously chosen zzso We present a zzso difference based learning rule for training the weights from these additional zzso to the state zzso As a result, the performance of the network is maintained both, in the presence of zzso features, and at randomly occurring time steps during which the input is zzso Interestingly, a zzso forward model emerges from the memory zzso which only covers the zzso pairs that are relevant to the zzso The model presents a link between reinforcement learning, feature detection and forward models and may help to explain how reward systems recruit cortical circuits for zzso feature detection and zzso 

