
In this article we prove zzso bounds for the error of the zzso zzso estimate of the zzso zzso that is, bounds showing that the worst-case error of this estimate is not much worse than that of the training error zzso The name sanity check refers to the fact that although we often expect the zzso estimate to perform considerably better than the training error estimate, we are here only seeking assurance that its performance will not be considerably zzso Perhaps surprisingly, such assurance has been given only for limited cases in the prior literature on zzso Any zzso bound on the error of zzso must rely on some notion of zzso zzso Previous bounds relied on the rather strong notion of hypothesis stability, whose application was primarily limited to zzso and other local zzso Here we introduce the new and weaker notion of error stability and apply it to obtain zzso bounds for zzso for other classes of learning zzso including training error zzso procedures and zzso zzso We also provide lower bounds demonstrating the necessity of some form of error stability for proving bounds on the error of the zzso estimate, and the fact that for training error zzso zzso in the worst case such bounds must still depend on the zzso dimension of the hypothesis zzso 

