
Since cameras blur the incoming light during zzso different images of the same surface do not contain the same information about that zzso Thus, in general, corresponding points in multiple views of a scene have different image zzso While zzso geometry zzso the locations of corresponding points, it does not give relationships between the signals at corresponding zzso This paper offers an elementary treatment of these zzso We first develop the notion of zzso and zzso images, corresponding to, respectively, the raw incoming light and the measured zzso This framework separates the filtering and geometric aspects of zzso We then consider how to zzso one view of a surface from zzso if the transformation between the two views is zzso it emerges that this is possible if and only if the singular values of the zzso zzso are zzso Next, we consider how to combine the information in several views of a surface into a single output zzso By developing a new tool called zzso zzso we show how this can be done despite not knowing the blurring zzso 

