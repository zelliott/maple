
Approximate dynamic programming zzso is a class of reinforcement learning methods that have shown their importance in a variety of applications, including feedback control of zzso zzso zzso generally requires full information about the system internal states, which is usually not available in practical zzso In this paper, we show how to implement zzso methods using only measured zzso data from the zzso Linear zzso systems with deterministic behavior are considered zzso which are systems of great interest in the control system zzso In control system theory, these types of methods are referred to as output feedback zzso The zzso equivalent of the systems dealt with in this paper is a class of partially zzso zzso decision zzso We develop both policy iteration and value iteration zzso that zzso to an optimal controller that requires only zzso It is shown that, similar to Q zzso the new methods have the important advantage that knowledge of the system dynamics is not needed for the zzso of these learning zzso or for the zzso zzso Only the order of the system, as well as an upper bound on its zzso zzso must be zzso The learned zzso controller is in the form of a zzso zzso zzso controller that has equivalent performance with the optimal state variable feedback zzso 

