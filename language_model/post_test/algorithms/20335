
This article studies the zzso power of various zzso real zzso models that are based on the classical analog zzso neural network zzso This zzso consists of finite number of zzso each zzso zzso a zzso net function and a zzso continuous zzso zzso We introduce zzso networks as zzso zzso with a few simple zzso zzso threshold or zero zzso zzso We argue that even with weights restricted to zzso time zzso zzso zzso networks are able to zzso arbitrarily complex zzso zzso We identify many types of neural networks that are at least as powerful as zzso zzso some of which are not in fact zzso but they boost other zzso operations in the net function zzso zzso that can use divisions and zzso net functions inside zzso continuous zzso zzso These zzso networks are equivalent to the zzso model, when the latter is restricted to a bounded number of zzso With respect to zzso on digital computers, we show that zzso networks with rational weights can be simulated with exponential precision, but even with zzso zzso real zzso zzso networks are not subject to any fixed precision zzso This is in contrast with the zzso that are known to demand precision that is linear in the zzso zzso When zzso periodic functions zzso fractional part, zzso zzso are added to zzso networks, the resulting networks are zzso equivalent to a zzso parallel zzso Thus, these highly zzso networks can solve the presumably intractable class of zzso problems in zzso zzso 

