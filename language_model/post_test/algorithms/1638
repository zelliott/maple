
This paper studies the design and application of a novel visual attention model designed to zzso user's gaze position automatically, zzso without using a zzso zzso The model we propose is specifically designed for real-time first-person exploration of zzso virtual zzso It is the first model adapted to this context which can zzso in real time a continuous gaze point position instead of a set of zzso objects potentially observed by the zzso To do so, contrary to previous models which use a zzso representation of visual objects, we introduce a representation based on zzso Our model also simulates visual zzso and the cognitive processes which take place in the brain such as the gaze behavior associated to first-person navigation in the virtual zzso Our visual attention model combines both bottom-up and zzso components to zzso a continuous gaze point position on screen that hopefully matches the user's zzso We conducted an experiment to study and compare the performance of our method with a state-of-the-art zzso Our results are found significantly better with sometimes more than 100 percent of accuracy zzso This suggests that computing a gaze point in a zzso virtual environment in real time is possible and is a valid approach, compared to zzso zzso Finally, we expose different applications of our model when exploring virtual zzso We present different zzso which can improve or adapt the visual feedback of virtual environments based on gaze zzso We first propose a zzso approach that heavily relies on zzso zzso We show that it is possible to use the gaze information of our visual attention model to increase visual quality where the user is looking, while maintaining a zzso zzso Second, we introduce the use of the visual attention model in three visual effects inspired by the human visual system zzso zzso zzso zzso motions, and dynamic zzso All these effects are zzso based on the simulated gaze of the user, and are meant to improve user's sensations in future virtual reality zzso 

