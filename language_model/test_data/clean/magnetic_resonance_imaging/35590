
The pattern of intonation accompanying an utterance provides a powerful cue as to a speaker's emotional state of mind. Most prior lesion studies have demonstrated that the nodal point for decoding these prosodic emotion cues is mediated by unimodal auditory cortex in the right posterior lateral temporal lobe. However, functional neuroimaging has brought with it increasing attention to the equivalent left hemisphere region in this role. This study used fMRI to quantitatively assess the hypothesis that involvement of the left posterior lateral temporal lobe depended on the linguistic load or verbal complexity of the prosodic emotion stimuli. BOLD contrast data was acquired on a 3T scanner whilst 16 healthy young adults identified the prosodic emotion in three conditions: 'sentences' comprised of words, a repeated monosyllable, and a single prolonged syllable (asyllabic). Whole-brain analyses were performed using SPM5 and supplemented by posterior lateral temporal lobe region of interest (ROI) analyses. The whole-brain analyses appeared to show bilateral temporal lobe activation across the conditions, however, the ROI analyses indicated a highly significant decrease in activity in the left ROI as verbal complexity decreased. Changes in right ROI activity were not statistically significant. Our results indicate that the likelihood of observing a notable left temporal lobe response in functional neuroimaging studies of emotional prosody comprehension depends on the verbal complexity of the prosodic emotion stimuli. Despite the right hemisphere dominance underlying this task, the left hemisphere region may be co-activated in its attempt to extract phonetic-segmental information from the acoustic stimuli whether or not the stimuli contain meaningful phonetic-segmental information.

