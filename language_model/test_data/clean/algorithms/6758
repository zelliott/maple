
The relative success of chemical oxygen demand (COD) removal models to describe measured rates of COD removal in a pilot-scale constructed wetland designed for treatment of high-strength winery wastewater are evaluated using retention times determined from tracer studies. Not surprisingly, two-parameter residual and retardation models better fit the measured removal data than single-parameter, first-order decay models for wastewater at average COD loadings up to nearly 5000 mg/L. The residual and retardation models yielded nearly equivalent fits to the measured data. However, the retardation model had more consistent parameters for COD removal data across different depth levels in the constructed wetland and at different loadings, and a slightly smaller sum of least-squared errors. The retardation model seems to be appropriate for constructed wetland design because it allows a steady decrease in COD with increased treatment time rather than a constant residual COD (C*) value. From the least-squares optimization procedure used to estimate model parameters (a volumetric rate constant, Kv, range of 3 to 12 d(-1)), nonrealistic, or physically meaningless, large C* values (C* range of 23 to 450 mg COD/L) that were dependent on COD loading were obtained, potentially underestimating the constructed wetland system's actual winery wastewater treatment potential. The optimal parameters for the retardation model applied to the pilot-scale constructed wetland ranged from 9 to 12 d(-1) for the initial degradation rate constant, Ko, and 2 to 5 d(-1) for the time-based retardation coefficient, b. These values should be verified for full-scale field systems based on field measurements currently underway.

