
We develop a model of cortical coding of stimuli by the sequences of activation patterns that they ignite in an initially random network. Hebbian learning then stabilizes these sequences, making them attractors of the dynamics. There is a competition between the capacity of the network and the stability of the sequences; for small stability parameter epsilon (the strength of the mean stabilizing PSP in the neurons in a learned sequence) the capacity is proportional to 1/epsilon 2. For epsilon of the order of or less than the PSPs of the untrained network, the capacity exceeds that for sequences learned from tabula rasa.

