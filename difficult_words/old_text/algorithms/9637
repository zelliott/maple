
The Adachi neural network (AdNN) is a fascinating neural network (NN) which has been shown to possess chaotic properties, and to also demonstrate associative memory (AM) and pattern recognition (PR) characteristics. Variants of the AdNN have also been used to obtain other PR phenomena, and even blurring. An unsurmountable problem associated with the AdNN and the variants referred to above is that all of them require a quadratic number of computations. This is essentially because the NNs in each case are completely connected graphs. In this paper, we consider how the computations can be significantly reduced by merely using a linear number of computations. To achieves this, we extract from the original completely connected graph one of its spanning trees. We then address the problem of computing the weights for this spanning tree. This is done in such a manner that the modified tree-based NN has approximately the same input-output characteristics, and thus the new weights are themselves calculated using a gradient-based algorithm. By a detailed experimental analysis, we show that the new linear-time AdNN-like network possesses chaotic and PR properties for different settings. As far as we know, such a tree-based AdNN has not been reported, and the results given here are novel.

